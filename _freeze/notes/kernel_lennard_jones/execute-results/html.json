{
  "hash": "04e40a75b227568d9de274e1de99f956",
  "result": {
    "markdown": "---\ntitle: Kernel learning of a Lennard-Jones potential\nauthor: Tristan Bereau\ndate: 'Mar 28, 2023'\nformat:\n  html:\n    code-fold: false\nkeep-ipynb: true\n---\n\nLet's learn a Lennard-Jones potential (shown in @fig-lj-plot). We'll use a Gaussian process regression aka kernel. Later we will have a look at the effect of a physically motivated prior.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import mean_absolute_error\nfrom dataclasses import dataclass, field\nfrom typing import Literal\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef lj(r):\n    return 4. * ((1./r)**12 - (1./r)**6)\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndistances = np.linspace(0.9, 2.0, 50)\n\nX_train, X_test = train_test_split(distances, train_size=0.5, shuffle=True)\ny_train, y_test = lj(X_train), lj(X_test)\nX_train = X_train.reshape((len(X_train), 1))\nX_test = X_test.reshape((len(X_test), 1))\n\n\nplt.plot(X_train, lj(X_train), 'or', label='Training')\nplt.plot(X_test, lj(X_test), 'og', label='Test')\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Interatomic distance\")\nplt.ylabel(\"Lennard-Jones energy\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Lennard-Jones potential as a function of distance](kernel_lennard_jones_files/figure-html/fig-lj-plot-output-1.png){#fig-lj-plot width=587 height=434}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n@dataclass\nclass GaussianKernel:\n    sigma: float\n    regularization: float\n    norm: Literal[\"euclidean\", \"cityblock\"]\n    alpha: np.array = field(init=False)\n    training_set: np.array = field(init=False)\n\n    def __post_init__(self):\n        self.alpha = None\n        self.training_set = None\n\n    def get_kernel_matrix(self, x_1: np.array, x_2: np.array) -> np.array:\n        return np.exp(- cdist(x_1, x_2, self.norm) / self.sigma)\n\n    def fit(self, x: np.array, y: np.array) -> None:\n        self.training_set = x\n        regularized_kernel = (\n                self.get_kernel_matrix(x, x)\n                + self.regularization * np.identity(len(x))\n        )\n        self.alpha = np.dot(y, np.linalg.inv(regularized_kernel))\n\n    def predict(self, x: np.array) -> np.array:\n        assert self.alpha is not None, \"Model has not been trained yet!\"\n        return np.dot(self.alpha, self.get_kernel_matrix(self.training_set, x))\n\n    def mean_absolute_error(self, x: np.array, y: np.array) -> float:\n        return mean_absolute_error(y, self.predict(x))\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ngpr = GaussianKernel(sigma=0.1, regularization=1e-9, norm=\"cityblock\")\ngpr.fit(X_train, y_train)\ny_predict = gpr.predict(X_test)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.plot(X_test, y_test, 'or', label='Reference')\nplt.plot(X_test, y_predict, 'og', label='Predictions')\nplt.axhline(0, linestyle=\"--\")\nplt.legend()\nplt.xlabel(\"Interatomic distance\")\nplt.ylabel(\"Lennard-Jones energy\")\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](kernel_lennard_jones_files/figure-html/cell-7-output-1.png){width=587 height=429}\n:::\n:::\n\n\nThe predictions do well overall, except in the repulsive region, where they severely underestimate. We'll improve upon this later by adding some prior information.\n\nFor now, let's compute a learning curve to better assess the quality of the model. We'll do this by averaging several ML models for a number of training set sizes.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ngpr.mean_absolute_error(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.17720286396547613\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n@dataclass\nclass LearningCurve:\n    gpr: GaussianKernel\n    input_x: np.array\n    target_y: np.array\n    number_samples: int = 100\n\n    def score(self, training_ratio: float) -> pd.Series:\n        maes = []\n        for i in range(self.number_samples):\n            x_train_, x_test_ = train_test_split(\n                distances, train_size=training_ratio, shuffle=True\n            )\n            y_train_, y_test_ = lj(x_train_), lj(x_test_)\n            x_train_ = x_train_.reshape((len(x_train_), 1))\n            x_test_ = x_test_.reshape((len(x_test_), 1))\n            self.gpr.fit(x_train_, y_train_)\n            maes.append(self.gpr.mean_absolute_error(x_test_, y_test_))\n        return pd.Series(data=maes, name=f\"{training_ratio}\")\n\n    def scores(self, training_ratios: list[float]) -> pd.DataFrame:\n        return pd.concat([self.score(ratio) for ratio in training_ratios], axis=1)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlearning_curve = LearningCurve(gpr, distances, lj(distances), number_samples=500)\ndf_scores = learning_curve.scores(\n    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n).describe()\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.errorbar(\n    x=df_scores.columns.values,\n    y=df_scores.T[\"mean\"].values,\n    yerr=df_scores.T[\"std\"].to_numpy() / np.sqrt(df_scores.T[\"count\"].to_numpy())\n)\nplt.loglog()\nplt.xlabel(\"Training ratio\")\nplt.ylabel(\"Mean absolute error\")\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](kernel_lennard_jones_files/figure-html/cell-11-output-1.png){width=632 height=431}\n:::\n:::\n\n\n## Adding a prior to the potential\n\nPredictions at short range tend to fail badly, due to difficulties in efficiently learning the divergence. To alleviate this, let's implement a simple linear repulsive prior of the form\n$$\n U_{\\rm prior} = -\\gamma(r-r_0)\\theta(r_0-r),\n$$\nwhere $\\gamma$ and $r_0$ are new hyperparameters, which control the slope and range of the function, respectively. $\\theta$ denotes the Heaviside step function (returns 0 if the argument is negative, 1 otherwise).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n@dataclass\nclass GaussianKernelWithPrior(GaussianKernel):\n    gamma: float\n    r_0: float\n\n    def prior(self, r):\n        return -self.gamma * (r - self.r_0) * np.heaviside(self.r_0 - r, 0.)\n\n    def fit(self, x: np.array, y: np.array) -> None:\n        self.training_set = x\n        regularized_kernel = (\n                self.get_kernel_matrix(x, x)\n                + self.regularization * np.identity(len(x))\n        )\n        y_shifted = y - self.prior(x.T[0])\n        self.alpha = np.dot(y_shifted, np.linalg.inv(regularized_kernel))\n\n    def predict(self, x: np.array) -> np.array:\n        assert self.alpha is not None, \"Model has not been trained yet!\"\n        return (\n            self.prior(x.T[0])\n            + np.dot(self.alpha, self.get_kernel_matrix(self.training_set, x))\n        )\n\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ngpr_w_prior = GaussianKernelWithPrior(\n    sigma=0.1, regularization=1e-9, norm=\"cityblock\", gamma=150., r_0=0.95\n)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ngpr_w_prior.fit(X_train, y_train)\ngpr_w_prior.mean_absolute_error(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n0.22177868902300013\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nlearning_curve_w_prior = LearningCurve(\n    gpr_w_prior, distances, lj(distances), number_samples=500\n)\ndf_scores_w_prior = (\n    learning_curve_w_prior\n    .scores([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95])\n    .describe()\n)\n\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.errorbar(\n    x=df_scores.columns.values,\n    y=df_scores.T[\"mean\"].values,\n    yerr=(\n        df_scores.T[\"std\"].to_numpy()\n        / np.sqrt(df_scores.T[\"count\"].to_numpy())\n    ),\n    label=\"without\",\n)\nplt.errorbar(\n    x=df_scores_w_prior.columns.values,\n    y=df_scores_w_prior.T[\"mean\"].values,\n    yerr=(\n        df_scores_w_prior.T[\"std\"].to_numpy()\n        / np.sqrt(df_scores_w_prior.T[\"count\"].to_numpy())\n    ),\n    label=\"with prior\",\n)\nplt.loglog()\nplt.legend()\nplt.xlabel(\"Training ratio\")\nplt.ylabel(\"Mean absolute error\")\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](kernel_lennard_jones_files/figure-html/cell-16-output-1.png){width=632 height=431}\n:::\n:::\n\n\nLearning with this simple repulsive prior leads to improved learning performance: while the slope (i.e., learning rate) is roughly the same, the offset is significantly reduced. Note that this is a log-log plot.\n\nStatistical information theory predicts that ML models often learn with a power-law behavior between test error and training-set size\n$$\nE \\sim \\beta N^\\nu\n$$\nwhere the coefficients $\\beta$ and $\\nu$ dictate the offset and slope of learning, respectively. See papers from Vapnik et al. for more details.\n\n\n",
    "supporting": [
      "kernel_lennard_jones_files"
    ],
    "filters": [],
    "includes": {}
  }
}